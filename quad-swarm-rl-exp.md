# QuadSwarm-RL 项目实验经历

## 项目概述
quad-swarm-rl是一个无人机强化学习环境，使用了sample-factory的框架，支持多无人机飞行，主要任务是无人机飞向目标点，可以设置障碍物。

## 主要实验与技术实现

### 1. 奖励函数设计与优化
- **奖励退火策略**：实现了渐进式奖励调整策略，通过在训练过程中动态调整碰撞惩罚系数，使智能体能够逐步适应更严格的约束条件。在训练初期给予较小的碰撞惩罚，使agent专注学习基本飞行技能，在后期逐步增加惩罚权重，使agent学会避障。
- **奖励成分分析**：系统性地分析了不同奖励成分对训练效果的影响，包括位置奖励(pos)、自旋奖励(spin)、姿态奖励(orient)和努力奖励(effort)：
  - pos+effort(13M): 有向目标跟进的迹象，但无法很好控制自身，会不断旋转，无法在目标点悬停
  - pos+spin+effort(13M): 可以短暂到达目标点且悬停，但会因机动需要做出较大roll或pitch动作导致翻转
  - pos+spin+orient(7M): 存在'用力过猛'现象，无法较好到达目标点
  - pos+spin+orient+effort(7M): 可以较好到达，但距离较远时存在'用力过猛'，会震荡后到达，偶尔会翻转
  - 经过奖励函数调整，使用e^-kx方式计算pos和spin奖励，最终确定3_0.2pos 1_1spin cost_orient = -np.exp(-5.0 * np.abs(-cost_orient_raw-1)) 0.05effort的组合，在100M步训练后能稳定完成任务，成功率高，deadlock稳定为0
  - 单一位置奖励实验表明，仅使用位置奖励也能完成任务，但可能伴随无意义的抖动和旋转

### 2. 网络架构优化
- **Actor-Critic权重共享**：对比了权重共享与非共享的训练效果，发现非共享权重在收敛速度、内存占用和训练时间方面表现更优，收敛速度快约1M步，内存占用平均少70MB，动作选择更加激进，标准差更低。
- **编码器对比实验**：比较了四种不同的编码方式(no_encoder、attention、mlp、mean_embed)在多智能体协调中的表现。在2-agent场景中，no_encoder收敛最快且性能最好，内存占用最小，但推测在更多智能体的场景中，attention机制可能更有优势。

### 3. 注意力机制实现
- 深入研究并实现了多头注意力机制，解决了传统RNN在处理长序列时的梯度消失问题，并提高了并行计算效率。
- 实现了缩放点积注意力公式：Attention(Q,K,V)=softmax(Q·K^T / sqrt(d_k)) · V
- 使用多头注意力Concat(head_1,...,head_n)W^o来增强模型的表达能力

### 4. 训练稳定性改进
- 发现并解决了CUDA环境下的内存泄漏问题，通过修改baseline.py和quad_multi_mix_baseline.py配置，切换到CPU训练模式以确保训练过程的稳定性。
- 应用了Xavier均匀分布初始化方法，有效缓解了深层网络训练中的梯度消失/爆炸问题。

### 5. 其他关键技术
- 实现了自定义Gym环境，包括定义观测和状态空间、reset、step、render等关键函数
- 探索了Signed Distance Field(SDF)在碰撞避免中的应用，发现可大致降低0.05的碰撞率，但作用有限

## 实验成果与发现
- 成功训练出能够在多智能体环境下完成目标导航任务的强化学习策略
- 通过大量实验验证了不同网络组件对最终性能的影响
- 发现单一位置奖励在足够长的训练时间内可以达到良好的导航效果，但结合多种奖励成分可提高飞行的平稳性
- 在2-agent场景中，no_encoder编码方式表现最佳，但在更多智能体场景中attention机制可能更优
- 奖励函数的精细调优对训练效果至关重要，经过精心设计的奖励函数可在5M-100M步内实现稳定收敛
- 为后续在复杂环境（含障碍物）中的实验奠定了基础